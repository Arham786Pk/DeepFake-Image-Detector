{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "NFkzQqAdDB0r",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "1b9d67c8-d806-464e-a74d-48904743bf2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QnhHdGVIRyS1"
   },
   "source": [
    "# **Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "Frm8STTpRx-0",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "e98aa628-49ca-450c-8098-ca545e509723"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing deepfake_detector.py\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "class DeepFakeDetector:\n",
    "    def __init__(self, data_dir, batch_size=32):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.setup_data_transforms()\n",
    "        self.setup_datasets()\n",
    "        self.setup_model()\n",
    "\n",
    "    def setup_data_transforms(self):\n",
    "        self.data_transforms = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomVerticalFlip(p=0.5),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        self.val_transforms = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def setup_datasets(self):\n",
    "        # Load datasets\n",
    "        dataset = datasets.ImageFolder(self.data_dir, transform=self.data_transforms)\n",
    "\n",
    "        # Calculate splits\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        val_size = len(dataset) - train_size\n",
    "\n",
    "        # Create splits\n",
    "        self.train_dataset, self.val_dataset = torch.utils.data.random_split(\n",
    "            dataset, [train_size, val_size]\n",
    "        )\n",
    "\n",
    "        # Update validation transform\n",
    "        self.val_dataset.dataset.transform = self.val_transforms\n",
    "\n",
    "        # Create dataloaders\n",
    "        self.train_loader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=2,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        self.val_loader = DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=2,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        self.classes = dataset.classes\n",
    "        print(f\"Classes: {self.classes}\")\n",
    "        print(f\"Total images: {len(dataset)}\")\n",
    "        print(f\"Training images: {len(self.train_dataset)}\")\n",
    "        print(f\"Validation images: {len(self.val_dataset)}\")\n",
    "\n",
    "    def setup_model(self):\n",
    "        # Load model with latest weights\n",
    "        weights = models.ResNet50_Weights.DEFAULT\n",
    "        self.model = models.resnet50(weights=weights)\n",
    "\n",
    "        # Freeze early layers\n",
    "        for param in list(self.model.parameters())[:-4]:\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Modify final layer for binary classification\n",
    "        num_ftrs = self.model.fc.in_features\n",
    "        self.model.fc = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 2)\n",
    "        )\n",
    "\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "        # Setup loss and optimizer\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.AdamW(self.model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, mode='min', patience=3, factor=0.1, verbose=True\n",
    "        )\n",
    "\n",
    "    def train_one_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch+1}')\n",
    "        for inputs, labels in pbar:\n",
    "            inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            pbar.set_postfix({'loss': loss.item(), 'acc': 100. * correct / total})\n",
    "\n",
    "        return running_loss / len(self.train_loader), 100. * correct / total\n",
    "\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(self.val_loader, desc='Validation'):\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        return (val_loss / len(self.val_loader),\n",
    "                100. * correct / total,\n",
    "                all_preds,\n",
    "                all_labels)\n",
    "\n",
    "    def train(self, num_epochs=10):\n",
    "        best_val_acc = 0.0\n",
    "        train_losses, train_accs = [], []\n",
    "        val_losses, val_accs = [], []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            # Training phase\n",
    "            train_loss, train_acc = self.train_one_epoch(epoch)\n",
    "            train_losses.append(train_loss)\n",
    "            train_accs.append(train_acc)\n",
    "\n",
    "            # Validation phase\n",
    "            val_loss, val_acc, all_preds, all_labels = self.validate()\n",
    "            val_losses.append(val_loss)\n",
    "            val_accs.append(val_acc)\n",
    "\n",
    "            print(f'\\nEpoch {epoch+1}/{num_epochs}:')\n",
    "            print(f'Training Loss: {train_loss:.4f} Acc: {train_acc:.2f}%')\n",
    "            print(f'Validation Loss: {val_loss:.4f} Acc: {val_acc:.2f}%')\n",
    "\n",
    "            # Update learning rate\n",
    "            self.scheduler.step(val_loss)\n",
    "\n",
    "            # Save best model\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'val_acc': val_acc,\n",
    "                }, 'best_deepfake_detector.pth')\n",
    "\n",
    "            # Print classification report\n",
    "            if epoch == num_epochs - 1:  # On last epoch\n",
    "                print(\"\\nClassification Report:\")\n",
    "                print(classification_report(all_labels, all_preds, target_names=self.classes))\n",
    "\n",
    "                # Plot confusion matrix\n",
    "                cm = confusion_matrix(all_labels, all_preds)\n",
    "                plt.figure(figsize=(8, 6))\n",
    "                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "                plt.title('Confusion Matrix')\n",
    "                plt.xlabel('Predicted')\n",
    "                plt.ylabel('True')\n",
    "                plt.show()\n",
    "\n",
    "        # Plot training history\n",
    "        plt.figure(figsize=(12, 4))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(train_losses, label='Train')\n",
    "        plt.plot(val_losses, label='Validation')\n",
    "        plt.title('Loss over time')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(train_accs, label='Train')\n",
    "        plt.plot(val_accs, label='Validation')\n",
    "        plt.title('Accuracy over time')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def predict_image(self, image_path):\n",
    "        \"\"\"Predict a single image\"\"\"\n",
    "        self.model.eval()\n",
    "        image = datasets.folder.default_loader(image_path)\n",
    "        image = self.val_transforms(image).unsqueeze(0).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(image)\n",
    "            probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        return {\n",
    "            'class': self.classes[predicted[0]],\n",
    "            'confidence': probabilities[0][predicted[0]].item(),\n",
    "            'probabilities': {\n",
    "                cls: prob.item()\n",
    "                for cls, prob in zip(self.classes, probabilities[0])\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Usage example:\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize detector\n",
    "    data_dir = \"/content/drive/MyDrive/deepfake-and-real-images/Dataset/Test\"\n",
    "    detector = DeepFakeDetector(data_dir)\n",
    "\n",
    "    # Train the model\n",
    "    detector.train(num_epochs=10)\n",
    "\n",
    "    # Example of prediction\n",
    "    result = detector.predict_image('/content/drive/MyDrive/deepfake-and-real-images/Dataset/Test/Fake/fake_0.jpg')\n",
    "    print(f\"Prediction: {result['class']}\")\n",
    "    print(f\"Confidence: {result['confidence']:.2f}\")\n",
    "    print(\"Class probabilities:\", result['probabilities'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CVJuy-EwU9un"
   },
   "source": [
    "# **Gradio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 723
    },
    "collapsed": true,
    "id": "xkAfibvCkrje",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "791263b6-83af-4384-bc07-0d7f1f85792a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Classes: ['Fake', 'Real']\n",
      "Total images: 10905\n",
      "Training images: 8724\n",
      "Validation images: 2181\n",
      "Loaded model from best_deepfake_detector.pth\n",
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://57fb481ab93597df32.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://57fb481ab93597df32.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "from deepfake_detector import DeepFakeDetector\n",
    "\n",
    "class GradioDeepFakeDetector:\n",
    "    def __init__(self, model_path, data_dir):\n",
    "        self.detector = DeepFakeDetector(data_dir)\n",
    "\n",
    "        if os.path.exists(model_path):\n",
    "            checkpoint = torch.load(model_path, map_location=self.detector.device, weights_only=True)\n",
    "            self.detector.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            print(f\"Loaded model from {model_path}\")\n",
    "        else:\n",
    "            print(\"No pre-trained model found. Please train the model first.\")\n",
    "\n",
    "        self.detector.model.eval()\n",
    "\n",
    "    def predict(self, image):\n",
    "        if image is None:\n",
    "            return \"Please upload an image to analyze.\", 0.0, None, None, None\n",
    "\n",
    "        temp_path = \"temp_image.jpg\"\n",
    "        image.save(temp_path)\n",
    "\n",
    "        try:\n",
    "            result = self.detector.predict_image(temp_path)\n",
    "\n",
    "            prediction = result['class']\n",
    "            confidence = result['confidence'] * 100\n",
    "\n",
    "            # Format detailed results\n",
    "            fake_prob = result['probabilities'].get('Fake', 0) * 100\n",
    "            real_prob = result['probabilities'].get('Real', 0) * 100\n",
    "\n",
    "            status = \"‚ö†Ô∏è LIKELY FAKE\" if prediction == \"Fake\" else \"‚úÖ LIKELY REAL\"\n",
    "\n",
    "            response = f\"### {status}\\n\\n\"\n",
    "            response += f\"**Overall Confidence:** {confidence:.1f}%\\n\\n\"\n",
    "\n",
    "            return (\n",
    "                response,\n",
    "                fake_prob/100,  # For the fake probability bar\n",
    "                real_prob/100,  # For the real probability bar\n",
    "                image,  # Return the analyzed image\n",
    "                f\"Analysis complete - {prediction} ({confidence:.1f}% confidence)\"  # Status message\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"Error processing image: {str(e)}\", 0.0, 0.0, None, \"Error occurred during analysis\"\n",
    "        finally:\n",
    "            if os.path.exists(temp_path):\n",
    "                os.remove(temp_path)\n",
    "\n",
    "def create_gradio_interface():\n",
    "    detector = GradioDeepFakeDetector(\n",
    "        model_path=\"best_deepfake_detector.pth\",\n",
    "        data_dir=\"/content/drive/MyDrive/deepfake-and-real-images/Dataset/Test\"\n",
    "    )\n",
    "\n",
    "    with gr.Blocks(\n",
    "        title=\"DeepFake Image Detector\",\n",
    "        theme=gr.themes.Soft(\n",
    "            primary_hue=\"blue\",\n",
    "            secondary_hue=\"gray\",\n",
    "        ),\n",
    "    ) as interface:\n",
    "        gr.Markdown(\"\"\"\n",
    "        # üîç DeepFake Image Detector\n",
    "\n",
    "        Upload an image to check if it's authentic or artificially generated.\n",
    "        Our AI model will analyze the image and provide detailed authenticity scores.\n",
    "\n",
    "        ---\n",
    "        \"\"\")\n",
    "\n",
    "        with gr.Row():\n",
    "            # Left column - Input\n",
    "            with gr.Column(scale=1):\n",
    "                input_image = gr.Image(\n",
    "                    label=\"üì§ Upload Image for Analysis\",\n",
    "                    type=\"pil\",\n",
    "                    height=400,\n",
    "                    show_label=True,\n",
    "                    container=True,\n",
    "                )\n",
    "\n",
    "                with gr.Row():\n",
    "                    submit_btn = gr.Button(\n",
    "                        \"üîç Analyze Image\",\n",
    "                        variant=\"primary\",\n",
    "                        size=\"lg\"\n",
    "                    )\n",
    "                    clear_btn = gr.Button(\n",
    "                        \"üóëÔ∏è Clear\",\n",
    "                        variant=\"secondary\",\n",
    "                        size=\"lg\"\n",
    "                    )\n",
    "\n",
    "                status_text = gr.Markdown(\n",
    "                    \"### Status: Waiting for image...\",\n",
    "                    elem_id=\"status_display\"\n",
    "                )\n",
    "\n",
    "            # Right column - Results\n",
    "            with gr.Column(scale=1):\n",
    "                output_image = gr.Image(\n",
    "                    label=\"Analyzed Image\",\n",
    "                    type=\"pil\",\n",
    "                    height=300,\n",
    "                    visible=True\n",
    "                )\n",
    "\n",
    "                results_text = gr.Markdown(\n",
    "                    label=\"Analysis Results\",\n",
    "                )\n",
    "\n",
    "                gr.Markdown(\"### Probability Distribution\")\n",
    "\n",
    "                with gr.Row():\n",
    "                    with gr.Column():\n",
    "                        fake_prob = gr.Slider(\n",
    "                            label=\"Fake Probability\",\n",
    "                            minimum=0,\n",
    "                            maximum=1,\n",
    "                            value=0,\n",
    "                            interactive=False,\n",
    "                            info=\"Likelihood of being artificially generated\"\n",
    "                        )\n",
    "\n",
    "                    with gr.Column():\n",
    "                        real_prob = gr.Slider(\n",
    "                            label=\"Real Probability\",\n",
    "                            minimum=0,\n",
    "                            maximum=1,\n",
    "                            value=0,\n",
    "                            interactive=False,\n",
    "                            info=\"Likelihood of being authentic\"\n",
    "                        )\n",
    "\n",
    "        # Add example images section\n",
    "        gr.Markdown(\"### Try with Example Images\")\n",
    "        if os.path.exists(\"examples\"):\n",
    "            gr.Examples(\n",
    "                examples=[\"/content/drive/MyDrive/deepfake-and-real-images/Dataset/Test/Fake/fake_7.jpg\", \"/content/drive/MyDrive/deepfake-and-real-images/Dataset/Test/Fake/fake_189.jpg\"],\n",
    "                inputs=input_image,\n",
    "                label=\"Example Images\",\n",
    "                examples_per_page=4\n",
    "            )\n",
    "\n",
    "        # Event handlers\n",
    "        submit_btn.click(\n",
    "            fn=detector.predict,\n",
    "            inputs=[input_image],\n",
    "            outputs=[results_text, fake_prob, real_prob, output_image, status_text]\n",
    "        )\n",
    "\n",
    "        clear_btn.click(\n",
    "            lambda: [None, 0.0, 0.0, None, \"### Status: Waiting for image...\"],\n",
    "            inputs=[],\n",
    "            outputs=[input_image, fake_prob, real_prob, output_image, status_text]\n",
    "        )\n",
    "\n",
    "    return interface\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo = create_gradio_interface()\n",
    "    demo.launch(\n",
    "        share=True,\n",
    "        show_error=True,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
